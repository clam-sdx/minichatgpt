{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c92b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbc2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from minichatgpt.experiments.imdb import config, sent_kwargs\n",
    "from minichatgpt import PPOTrainer, Lab\n",
    "\n",
    "# for the loss calculation\n",
    "from minichatgpt.core import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c0cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of the speed of this demonstration, the batch_size is temporarily decreased from 256 to 4\n",
    "batch_size = 4\n",
    "config.batch_size = batch_size\n",
    "config.forward_batch_size = batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea44cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2bd6a5d7d39a840d.arrow\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2ecf25d24c93f132.arrow\n"
     ]
    }
   ],
   "source": [
    "lab = Lab(config)\n",
    "dataset = lab.build_dataset(dataset_name=\"imdb\",input_min_text_length=2,input_max_text_length=8)\n",
    "new_policy, old_policy, tokenizer = lab.init_policies_tokenizer()\n",
    "lab.set_generation_config(do_sample=True,output_min_length=4,output_max_length=16,pad_token_id=tokenizer.eos_token_id)\n",
    "ppo_trainer = lab.init_ppo_trainer(config, new_policy, old_policy, tokenizer, dataset)\n",
    "reward_model = lab.init_reward_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b56fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carson/projects/minichatgpt/venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for batch_step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    \n",
    "    queries = batch['input_ids']\n",
    "    \n",
    "    #### Get response from gpt2\n",
    "    responses = []\n",
    "    for query in queries:\n",
    "        gen_len = lab.output_length_sampler()\n",
    "        lab.generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **lab.generation_kwargs)\n",
    "        responses.append(response.squeeze()[-gen_len:])\n",
    "\n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in responses]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q,r in zip(batch['query'], batch['response'])]\n",
    "    pipe_outputs = lab.reward_model(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    break\n",
    "    \n",
    "queries, responses, scores = ppo_trainer._step_safety_checker(batch_size, queries, responses, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b71eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 1.3289]), tensor([-0.0000, -0.0000, -0.0000, -1.8032]), tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 0.8134]), tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 2.2469])]\n"
     ]
    }
   ],
   "source": [
    "logprobs, ref_logprobs, values = ppo_trainer.batched_forward_pass(queries, responses)\n",
    "rewards, non_score_reward = ppo_trainer.compute_rewards(scores, logprobs, ref_logprobs)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0777af19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss/policy': tensor(-5.9605e-08, grad_fn=<MeanBackward0>),\n",
       " 'loss/value': tensor(1.5026, grad_fn=<MulBackward0>),\n",
       " 'loss/total': tensor(0.1503, grad_fn=<AddBackward0>),\n",
       " 'policy/entropy': tensor(4.1144, grad_fn=<MeanBackward0>),\n",
       " 'policy/approxkl': tensor(0., grad_fn=<MulBackward0>),\n",
       " 'policy/policykl': tensor(0., grad_fn=<MeanBackward0>),\n",
       " 'policy/clipfrac': tensor(0., dtype=torch.float64),\n",
       " 'policy/advantages': tensor([[-0.4301, -0.2099,  1.3914,  0.1723,  1.5478,  0.5608, -1.1211, -0.8782,\n",
       "          -1.0330]]),\n",
       " 'policy/advantages_mean': tensor(5.9605e-08),\n",
       " 'policy/ratio': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<ExpBackward0>),\n",
       " 'returns/mean': tensor(1.6401),\n",
       " 'returns/var': tensor(0.0263),\n",
       " 'val/vpred': tensor(1.8516, grad_fn=<MeanBackward0>),\n",
       " 'val/error': tensor(1.7348, grad_fn=<MeanBackward0>),\n",
       " 'val/clipfrac': tensor(0.5556, dtype=torch.float64),\n",
       " 'val/mean': tensor(2.9335),\n",
       " 'val/var': tensor(0.9115)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(range(config.batch_size))\n",
    "\n",
    "# train_minibatch()\n",
    "\n",
    "for idx in range(config.batch_size):\n",
    "    \n",
    "    loss_p, loss_v, train_stats = ppo_trainer.loss(\n",
    "        logprobs[idx].unsqueeze(0),\n",
    "        values[idx].unsqueeze(0),\n",
    "        rewards[idx].unsqueeze(0),\n",
    "        queries[idx].unsqueeze(0),\n",
    "        responses[idx].unsqueeze(0),\n",
    "        torch.cat([queries[idx],responses[idx]]).unsqueeze(0),\n",
    "    )\n",
    "    \n",
    "    loss = loss_p + loss_v\n",
    "    \n",
    "    ppo_trainer.optimizer.zero_grad()\n",
    "    ppo_trainer.accelerator.backward(loss)\n",
    "    ppo_trainer.optimizer.step()\n",
    "    \n",
    "    break\n",
    "    \n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d4434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.0357,  0.0763, -0.1024, -0.0261, -0.3684, -0.0090,  0.0406,  0.2354,\n",
      "         1.3726]), tensor([-5.0364e-04, -1.4275e-03, -2.0395e-03, -1.7986e+00]), tensor([-8.0247e-04,  1.3795e-03, -1.6917e-03, -1.8938e-03, -2.0363e-03,\n",
      "         9.4728e-04,  8.1354e-01]), tensor([ 2.6694e-03,  2.6340e-04, -3.4965e-03,  8.8003e-03,  1.5938e-03,\n",
      "         2.3842e-04, -2.5574e-03,  6.2203e-03,  5.3811e-03,  4.1922e-03,\n",
      "         9.3698e-03, -8.9775e-03,  1.1537e-02,  1.0396e-02,  2.2532e+00])]\n"
     ]
    }
   ],
   "source": [
    "logprobs, ref_logprobs, values = ppo_trainer.batched_forward_pass(queries, responses)\n",
    "rewards, non_score_reward = ppo_trainer.compute_rewards(scores, logprobs, ref_logprobs)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f286923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_logprobs_ = logprobs[idx].unsqueeze(0)\n",
    "values_ = values[idx].unsqueeze(0)\n",
    "rewards_ = rewards[idx].unsqueeze(0)\n",
    "queries_ = queries[idx].unsqueeze(0)\n",
    "responses_ = responses[idx].unsqueeze(0)\n",
    "model_input_ = torch.cat([queries[idx],responses[idx]]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b18983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 0.95\n",
      "gamma 1\n"
     ]
    }
   ],
   "source": [
    "print('lambda', ppo_trainer.config.lam)\n",
    "print('gamma',  ppo_trainer.config.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16554a02",
   "metadata": {},
   "source": [
    "#### Generalized advantage estimator\n",
    "\n",
    "The term lastgaelam maens last [generalized advantage estimator](https://arxiv.org/pdf/1506.02438.pdf) GAE lambda.\n",
    "There are many resources already for explaining what the [Adavantage](https://huggingface.co/blog/deep-rl-a2c) function is, so I will not go into it too much. In the equation below $A^{(k)}_{t}$ is the advantage that represents \"Compared to the average reward, or expected reward, reward we should get from state $ s_{t} $  till the end, How much more or less did we get specifically as a result of taking the action we took at step t rather than all the other actions we could have taken, not including the actions we took after or before step t\"\n",
    "\n",
    "##### Advantage = Returns - Values\n",
    "\n",
    "$A^{(1)}_{t}$ does this by incorporating only the actual reward r_t we got immediately after taking action $ a_{t} $ in $ s_{t} $ + $\\gamma V(s_{t+1})$ to estimate the rest of the future - $V(s_t)$, the expected total rewards averaged across all the action options at state s_t, good and bad. \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{(1)} &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(2)} &= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - V(s_t) \\\\\n",
    "\\cdots &= \\cdots \\\\ \n",
    "\\hat{A}_t^{(\\infty)} &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots - V(s_t)\n",
    "\\end{align}\n",
    "\n",
    "$A^{(2)}_{t}$ is similar to $A^{(1)}_{t}$, only we incorporate 2 steps of actual reward in the future then estimate the rest, and so on and so on.\n",
    "\n",
    "\n",
    "lam, or lambda $ \\lambda $ is the weight parameter, it is taught intuitively here in this other lesson about [ Exponentially Weighted Moving Average](https://medium.com/mlearning-ai/exponentially-weighted-average-5eed00181a09) (EWMA), only in this lesson it is called $ \\beta $. Basically, the higher $ \\lambda $ the more you are placing weight on values other than the most immediate one. \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3MIYRnLguhjvM0tr72wBA.png\" width=600 height=400>\n",
    "\n",
    "What you see is that lower the $ \\beta $ is, the more noisy the signal. Thats because the lower the beta the less we are taking into account the more stable past values, instead changing the moving avareg alot based on the most recent volatile new piece of data. With higher beta we are weighing the past known and now static values more heavily, thereby inducing a smoother curve.\n",
    "\n",
    "However, and im sorry for doing this, but with respect to GAE, the situation is reversed int both ways, from the example shown in the graph. So why did I show it to you? Well the example is easier to understand and the relationship is similar only reversed, and the relationship is harder to describe. But once you see that relationship, I think its easier to take the inverse of a relatshiption you do understand, than to explain the relationship is a more confusing setting. \n",
    "\n",
    "Whereas in typical times series EWMA, the most immediate data is the most recently data in the past and the other data is the data in the farther past, in GAE the most immediate data is the next reward and the other data is the rewards we are estimating we might get in the future, via the state value function V(s). The $ \\lambda $ is therefore higher when you want to weight these far future estimates higher at the expense of the ones in your immediate future which are more certain. \n",
    "\n",
    "$\\hat{A}_t^{GAE(\\gamma,\\lambda)}$  is the generalized advantage estimator. This [Blog on GAE](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/) explains it well. The higher lambda is the more future steps (k's) you are taking into account\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{GAE(\\gamma,\\lambda)} &= (1-\\lambda)\\Big(\\hat{A}_{t}^{(1)} + \\lambda \\hat{A}_{t}^{(2)} + \\lambda^2 \\hat{A}_{t}^{(3)} + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\Big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V)+ \\cdots \\Big)  \\\\\n",
    "&= (1-\\lambda)\\Big( \\delta_t^V(1+\\lambda+\\lambda^2+\\cdots) + \\gamma\\delta_{t+1}^V(\\lambda+\\lambda^2+\\cdots) + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\left(\\delta_t^V \\frac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V\\frac{\\lambda}{1-\\lambda} + \\cdots\\right) \\\\\n",
    "&= \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^{V}\n",
    "\\end{align}\n",
    "\n",
    "***\n",
    "The tradeoff here is that the estimators $A^{(k)}_{t}$ with small k have low variance but high bias, whereas those with large k have low bias but high variance. Why?\n",
    "\n",
    "I think of it based on the number of terms. With small k, we have fewer terms to sum over (which means low variance). However, the bias is relatively large because it does not make use of extra “exact” information with r_K for K > k\n",
    "\n",
    "Here’s another way to think of it as emphasized in the paper: V(s_t)\n",
    "is constant among the estimator class, so it does not affect the relative bias or variance among the estimators: differences arise entirely due to the k -step returns.\n",
    "***\n",
    "\n",
    "In RL and machine learning, we are calling this noise, the variance, as in the bias variance tradeoff.\n",
    "\n",
    "I like to sum this up as \"the L in lambda for for longtermism\" and depending on the choices we make today, there are many variants of the future we could end up in, so larges L means more longterms and more variance. \n",
    "\n",
    "Basically like many tradeoffs there exists a point of balance for your particular problem. Like in the below example, you get bad learning not only when lambda is too high, but also when it is too low.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/ca11ba7b2991fe07b7a99b3a3aeba2486ed36261/9-Figure4-1.png\">\n",
    "\n",
    "Im going to rewrite the set of equations above to better mirror the code that we will implement below;\n",
    "\n",
    "first lets add the lambda term\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{(1)} &= \\delta^{V}_{t} = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(2)} &= \\delta^{V}_{t} + (\\gamma \\lambda) \\delta^{V}_{t+1} \\\\\n",
    "\\hat{A}_t^{(k)} &= \\sum_{l=0}^{l=k} (\\gamma \\lambda)^l \\delta_{t+l}^{V}\n",
    "\\end{align}\n",
    "\n",
    "next, rewrite ${A}_t^{(T - t)}$ in terms of t + 1, so that we can calculate the GAE for each\n",
    "step t in the sequence from the last T to the first 0:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^{V}_{t} &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(T - t)} &= \\delta^{V}_{t} + (\\gamma \\lambda) \\delta^{V}_{t+1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The above two expressions correspond to line 2 and 3 below respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9c65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0357,  0.0763, -0.1024, -0.0261, -0.3684, -0.0090,  0.0406,  0.2354,\n",
      "          1.3726]]) torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "print(rewards_, rewards_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ef43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6959, 3.1677, 1.7884, 2.3545, 1.2983, 2.5610, 4.0929, 0.2901, 3.5888]]) torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "print(values_, values_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bda2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_len 9\n"
     ]
    }
   ],
   "source": [
    "# rewards_ and values_\n",
    "\n",
    "gen_len = rewards_.shape[-1]\n",
    "\n",
    "print('gen_len', gen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aedd6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.0 tensor([-2.2162])\n",
      "7 tensor([3.5888]) tensor([1.4287])\n",
      "6 tensor([0.2901]) tensor([-2.4050])\n",
      "5 tensor([4.0929]) tensor([-0.7618])\n",
      "4 tensor([2.5610]) tensor([0.1706])\n",
      "3 tensor([1.2983]) tensor([-0.9203])\n",
      "2 tensor([2.3545]) tensor([-0.4106])\n",
      "1 tensor([1.7884]) tensor([-1.6930])\n",
      "0 tensor([3.1677]) tensor([-2.1009])\n",
      " \n",
      "[tensor([-2.2162]), tensor([1.4287]), tensor([-2.4050]), tensor([-0.7618]), tensor([0.1706]), tensor([-0.9203]), tensor([-0.4106]), tensor([-1.6930]), tensor([-2.1009])]\n",
      " \n",
      "tensor([[-2.1009, -1.6930, -0.4106, -0.9203,  0.1706, -0.7618, -2.4050,  1.4287,\n",
      "         -2.2162]])\n"
     ]
    }
   ],
   "source": [
    "lastgaelam = 0\n",
    "advantages_reversed = []\n",
    "\n",
    "# iterate backwards from last time step of episode, t = T -> 0 \n",
    "for t in reversed(range(gen_len)):\n",
    "    \n",
    "    # 1. V(s_t+1) for all t except t = T\n",
    "    nextvalues = values_[:, t + 1] if t < gen_len - 1 else 0.0  \n",
    "    \n",
    "    # 2. delta_t =  r_t + gamma*V(s_t+1) - V(s_t) \n",
    "    delta = rewards_[:, t] + ppo_trainer.config.gamma * nextvalues - values_[:, t]\n",
    "    \n",
    "    # 3. A_t = delta_t + gamma*lambda*delta_t+1\n",
    "    lastgaelam = delta + ppo_trainer.config.gamma * ppo_trainer.config.lam * lastgaelam\n",
    "    \n",
    "    advantages_reversed.append(lastgaelam)\n",
    "    \n",
    "    print(t, nextvalues, lastgaelam)\n",
    "    \n",
    "  \n",
    "print(' ')\n",
    "print(advantages_reversed)\n",
    "print(' ')\n",
    "\n",
    "# reverse the reverse to make regular and concatenate\n",
    "advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
    "\n",
    "print(advantages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e7f70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5950, 1.4747, 1.3778, 1.4342, 1.4689, 1.7992, 1.6879, 1.7188, 1.3726]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = advantages + values_\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669a58c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8776, -0.5554,  0.4575,  0.0549,  0.9165,  0.1801, -1.1177,  1.9102,\n",
       "         -0.9687]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whitening simply subtracts the means and divides by the standard deviation to zero mean the data and \n",
    "# impose a std of 1\n",
    "advantages = whiten(advantages)\n",
    "advantages = advantages.detach()\n",
    "\n",
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57356878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -15.2306,  -15.5122,  -18.7092,  ...,  -23.0487,  -20.8415,\n",
       "           -15.0127],\n",
       "         [ -56.4440,  -57.9299,  -64.8926,  ...,  -66.4360,  -65.7871,\n",
       "           -59.8450],\n",
       "         [ -43.0327,  -43.6636,  -47.6720,  ...,  -52.2125,  -53.2918,\n",
       "           -44.0718],\n",
       "         ...,\n",
       "         [ -42.4859,  -43.6363,  -45.9099,  ...,  -50.5001,  -45.3121,\n",
       "           -43.1839],\n",
       "         [ -39.3319,  -40.4079,  -44.4785,  ...,  -48.8025,  -42.4249,\n",
       "           -40.2178],\n",
       "         [-251.6243, -249.9496, -256.8471,  ..., -275.3591, -283.3862,\n",
       "          -251.1748]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# didnt we do this step already within the bacthed forwardf pass? Yes, we did, this is a redundant step\n",
    "input_kwargs = {\"input_ids\": model_input_}\n",
    "logits, _, vpred = ppo_trainer.model(**input_kwargs)\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4aac",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7389bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 50257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-15.2306, -15.5122, -18.7092,  ..., -23.0487, -20.8415, -15.0127],\n",
       "        [-56.4440, -57.9299, -64.8926,  ..., -66.4360, -65.7871, -59.8450],\n",
       "        [-43.0327, -43.6636, -47.6720,  ..., -52.2125, -53.2918, -44.0718],\n",
       "        [-52.4146, -51.3557, -54.4170,  ..., -60.5352, -60.4881, -49.1377],\n",
       "        [-50.6890, -49.8059, -52.7180,  ..., -59.1432, -58.8262, -48.2836]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = ppo_trainer.data_collator(batch[\"input_ids\"])[\"input_ids\"]\n",
    "input_kwargs = {\"input_ids\": input_ids}\n",
    "logits, loss, values = new_policy(**input_kwargs)\n",
    "print(logits.shape)\n",
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b2633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
