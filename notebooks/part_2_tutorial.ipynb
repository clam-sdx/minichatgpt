{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b884b9a1",
   "metadata": {},
   "source": [
    "#### PPO RL Algorithm with Natural Language Sequences\n",
    "\n",
    "The RL update step uses the log probabilities, value function estimations, stepwise rewards, the prompt tokens and generated tokens \n",
    "\n",
    "Lets first just quickly go thru all the steps from part 1 to get to the same log probabilities, value function estimations, stepwise rewards, the prompt tokens and generated tokens in order to continue from where we left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c92b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbc2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from minichatgpt.experiments.imdb import config, sent_kwargs\n",
    "from minichatgpt import Lab\n",
    "from minichatgpt.processdata.collators import imdb_dataloader_collator\n",
    "\n",
    "# for the loss calculation\n",
    "from minichatgpt.core import whiten, logprobs_from_logits, clip_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c0cc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the sake of the speed of this demonstration, the batch_size is temporarily decreased from 256 to 4\n",
    "batch_size = 4\n",
    "config.batch_size = batch_size\n",
    "config.forward_batch_size = batch_size//2\n",
    "config.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea44cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-57d76e4722ace3a5.arrow\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-26d56c39f81904d0.arrow\n"
     ]
    }
   ],
   "source": [
    "lab = Lab(config)\n",
    "dataset = lab.build_dataset(dataset_name=\"imdb\",input_min_text_length=2,input_max_text_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbe7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy, old_policy, tokenizer = lab.init_policies_tokenizer()\n",
    "lab.set_generation_config(do_sample=True,output_min_length=4,output_max_length=16,pad_token_id=tokenizer.eos_token_id)\n",
    "ppo_trainer = lab.init_ppo_trainer(\n",
    "    config, \n",
    "    new_policy,old_policy, \n",
    "    tokenizer, \n",
    "    dataset, dataloader_collator=imdb_dataloader_collator,\n",
    ")\n",
    "reward_model = lab.init_reward_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b56fd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-1.1495), tensor(-1.1267), tensor(-2.4217), tensor(-0.2299)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch_step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    \n",
    "    queries = batch['input_ids']\n",
    "    \n",
    "    #### Get response from gpt2\n",
    "    responses = []\n",
    "    for query in queries:\n",
    "        gen_len = lab.output_length_sampler()\n",
    "        lab.generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **lab.generation_kwargs)\n",
    "        responses.append(response.squeeze()[-gen_len:])\n",
    "\n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in responses]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q,r in zip(batch['query'], batch['response'])]\n",
    "    pipe_outputs = lab.reward_model(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    break\n",
    "    \n",
    "queries, responses, scores = ppo_trainer._step_safety_checker(batch_size, queries, responses, rewards)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8f49c",
   "metadata": {},
   "source": [
    "Telow we will study whats going on in `ppo_trainer.compute_logits_vpred` and `ppo_trainer.loss`. As you can see below, when you run these together, even with one iteration, the policy will be updated such that the rewards will have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9996d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -1.1495]), tensor([-0.0000, -0.0000, -0.0000, -1.1267]), tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -2.4217]), tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.2299])]\n"
     ]
    }
   ],
   "source": [
    "old_logprobs, ref_logprobs, values = ppo_trainer.batched_forward_pass(queries, responses)\n",
    "\n",
    "rewards, non_score_reward = ppo_trainer.compute_rewards(scores, old_logprobs, ref_logprobs)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0777af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1351, -0.1932, -0.1856,  0.1601, -1.8373, -0.3569, -0.7255, -1.1487,\n",
      "         -0.0072]]) 0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss/policy': tensor(5.2982e-08, grad_fn=<MeanBackward0>),\n",
       " 'loss/value': tensor(0.6233, grad_fn=<MulBackward0>),\n",
       " 'loss/total': tensor(0.0623, grad_fn=<AddBackward0>),\n",
       " 'policy/entropy': tensor(4.1144, grad_fn=<MeanBackward0>),\n",
       " 'policy/approxkl': tensor(0., grad_fn=<MulBackward0>),\n",
       " 'policy/policykl': tensor(0., grad_fn=<MeanBackward0>),\n",
       " 'policy/clipfrac': tensor(0., dtype=torch.float64),\n",
       " 'policy/advantages': tensor([[ 0.9538, -0.4945, -0.5671, -1.1690,  1.8384, -0.4001,  0.1166,  0.7457,\n",
       "          -1.0238]]),\n",
       " 'policy/advantages_mean': tensor(-5.2982e-08),\n",
       " 'policy/ratio': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.]], grad_fn=<ExpBackward0>),\n",
       " 'returns/mean': tensor(-1.0520),\n",
       " 'returns/var': tensor(0.0043),\n",
       " 'val/vpred': tensor(-0.8683, grad_fn=<MeanBackward0>),\n",
       " 'val/error': tensor(1.1707, grad_fn=<MeanBackward0>),\n",
       " 'val/clipfrac': tensor(0.3333, dtype=torch.float64),\n",
       " 'val/mean': tensor(-0.6033),\n",
       " 'val/var': tensor(0.4324)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = list(range(config.batch_size))\n",
    "\n",
    "# train_minibatch() # line 419 ppo_trainier.py\n",
    "\n",
    "for idx in range(config.batch_size):\n",
    "    \n",
    "    new_logprobs, vpred, logits = ppo_trainer.compute_logits_vpred(\n",
    "        model_input = torch.cat([queries[idx],responses[idx]]).unsqueeze(0), \n",
    "        query = queries[idx].unsqueeze(0), \n",
    "        response = responses[idx].unsqueeze(0), \n",
    "        rewards = rewards[idx].unsqueeze(0),\n",
    "    )\n",
    "    \n",
    "    loss_p, loss_v, train_stats = ppo_trainer.loss(\n",
    "        old_logprobs[idx].unsqueeze(0),\n",
    "        values[idx].unsqueeze(0),\n",
    "        rewards[idx].unsqueeze(0),\n",
    "        logits,\n",
    "        vpred,\n",
    "        new_logprobs,\n",
    "    )\n",
    "    \n",
    "    loss = loss_p + loss_v\n",
    "    \n",
    "    ppo_trainer.optimizer.zero_grad()\n",
    "    ppo_trainer.accelerator.backward(loss)\n",
    "    ppo_trainer.optimizer.step()\n",
    "    \n",
    "    break\n",
    "    \n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d4434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0639,  0.1145,  0.0327,  0.1879, -0.3708,  0.0094, -0.0191, -0.1769,\n",
      "        -1.0850]), tensor([-2.4188e-03, -3.4029e-04,  8.6808e-04, -1.1298e+00]), tensor([ 9.4826e-03, -2.3680e-04,  1.5378e-04, -1.5882e-03,  5.7784e-03,\n",
      "         5.8417e-04, -2.4220e+00]), tensor([ 0.0023,  0.0043, -0.0010,  0.0002,  0.0037, -0.0011,  0.0049,  0.0069,\n",
      "         0.0007,  0.0026,  0.0007, -0.0173,  0.0023,  0.0012, -0.2269])]\n"
     ]
    }
   ],
   "source": [
    "old_logprobs, ref_logprobs, values = ppo_trainer.batched_forward_pass(queries, responses)\n",
    "\n",
    "rewards, non_score_reward = ppo_trainer.compute_rewards(scores, old_logprobs, ref_logprobs)\n",
    "\n",
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ae08a",
   "metadata": {},
   "source": [
    "#### train_minibatch()\n",
    "\n",
    "train_minibatch does 3 things:\n",
    "\n",
    "1. compute_logits_vpred\n",
    "2. loss\n",
    "3. combine loss components and do backpropagation\n",
    "\n",
    "`ppo_trainer.compute_logits_vpred()` seems to do something very similar to `ppo_trainer.batched_forward_pass()` only it is for the new_policy only and returns the logits as well, the values are also shifted one position into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f286923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8163, -2.6540, -6.8816, -5.9944, -2.4721, -2.0452, -1.1314, -2.7215,\n",
      "         -5.6038]]) torch.Size([1, 9])\n",
      " \n",
      "tensor([[-1.1406, -0.2203, -0.2074,  0.1570, -1.6921, -0.3859, -0.7225, -1.1808,\n",
      "         -0.0697]]) torch.Size([1, 9])\n",
      " \n",
      "tensor([[-0.0639,  0.1145,  0.0327,  0.1879, -0.3708,  0.0094, -0.0191, -0.1769,\n",
      "         -1.0850]]) torch.Size([1, 9])\n",
      " \n",
      "tensor([[   40,  3505,  4964, 16089,   351,   337,     5,    38,    11,   290,\n",
      "           484,  6304]]) torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "# renaming our single sample to cleanly push this one sample through PPO for demonstration\n",
    "\n",
    "old_logprobs_ = old_logprobs[idx].unsqueeze(0)\n",
    "values_ = values[idx].unsqueeze(0)\n",
    "rewards_ = rewards[idx].unsqueeze(0)\n",
    "queries_ = queries[idx].unsqueeze(0)\n",
    "responses_ = responses[idx].unsqueeze(0)\n",
    "model_input_ = torch.cat([queries[idx],responses[idx]]).unsqueeze(0)\n",
    "\n",
    "print(old_logprobs_, old_logprobs_.shape)\n",
    "print(' ')\n",
    "print(values_, values_.shape) \n",
    "print(' ')\n",
    "print(rewards_, rewards_.shape)\n",
    "print(' ')\n",
    "print(model_input_, model_input_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107c77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1406, -0.2203, -0.2074,  0.1570, -1.6921, -0.3859, -0.7225, -1.1808,\n",
      "        -0.0697])\n",
      "tensor([[-8.8163, -2.6540, -6.8816, -5.9944, -2.4721, -2.0452, -1.1314, -2.7215,\n",
      "         -5.6038]], grad_fn=<SliceBackward0>) torch.Size([1, 9])\n",
      " \n",
      "tensor([[-0.2203, -0.2074,  0.1570, -1.6921, -0.3859, -0.7225, -1.1808, -0.0697,\n",
      "         -3.4668]], grad_fn=<SliceBackward0>) torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "print(values[0])\n",
    "\n",
    "new_logprobs_, vpred_, logits_ = ppo_trainer.compute_logits_vpred(model_input_, queries_, responses_, rewards_)\n",
    "\n",
    "print(new_logprobs_, new_logprobs_.shape)\n",
    "print(' ')\n",
    "print(vpred_, vpred_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fbc73",
   "metadata": {},
   "source": [
    "#### Advantage = Returns - Values\n",
    "\n",
    "The term `lastgaelam` meanns last [generalized advantage estimator](https://arxiv.org/pdf/1506.02438.pdf) GAE lambda.\n",
    "There are many resources already for explaining what the [Advantage](https://huggingface.co/blog/deep-rl-a2c) function is, so I will not go into it too much. In the equation below $A^{(k)}_{t}$ is the advantage that represents \"Compared to the average reward, or expected reward, reward we should get from state $ s_{t} $  till the end, How much more or less did we get specifically as a result of taking the action we took at step t rather than all the other actions we could have taken, not including the actions we took after or before step t\", so in a simple equation:\n",
    "\n",
    "$$ Advantage = Returns - Values $$\n",
    "\n",
    "`returns` is the total sum of rewards $ R(t) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots + \\gamma^T r_{T} $ where T is the total number of timesteps in the episode and gamma $ \\gamma $ is the discount factor that when small << 1 emphasizes short term rewards over long term rewards and when $ \\gamma $ = 1 weights longer term and short term equally.\n",
    "\n",
    "`values` is the model's prediction of what the returns will be at any given timestep t and state $s_t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b18983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 0.95 gamma 1\n"
     ]
    }
   ],
   "source": [
    "print('lambda', ppo_trainer.config.lam, 'gamma', ppo_trainer.config.gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16554a02",
   "metadata": {},
   "source": [
    "#### loss()\n",
    "\n",
    "This method is where the specifics of GAE and PPO are implemented.\n",
    "\n",
    "#### Generalized Advantage Estimator (GAE)\n",
    "\n",
    "$A^{(1)}_{t}$ does this by incorporating only the actual reward r_t we got immediately after taking action $ a_{t} $ in $ s_{t} $ + $\\gamma V(s_{t+1})$ to estimate the rest of the future - $V(s_t)$, the expected total rewards averaged across all the action options at state s_t, good and bad. \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{(1)} &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(2)} &= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - V(s_t) \\\\\n",
    "\\cdots &= \\cdots \\\\ \n",
    "\\hat{A}_t^{(\\infty)} &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots - V(s_t)\n",
    "\\end{align}\n",
    "\n",
    "$A^{(2)}_{t}$ is similar to $A^{(1)}_{t}$, only we incorporate 2 steps of actual reward in the future then estimate the rest, and so on and so on.\n",
    "\n",
    "\n",
    "lam, or lambda $ \\lambda $ is the weight parameter, it is taught intuitively here in this other lesson about [ Exponentially Weighted Moving Average](https://medium.com/mlearning-ai/exponentially-weighted-average-5eed00181a09) (EWMA), only in this lesson it is called $ \\beta $. Basically, the higher $ \\lambda $ the more you are placing weight on values other than the most immediate one. \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3MIYRnLguhjvM0tr72wBA.png\" width=600 height=400>\n",
    "\n",
    "What you see is that lower the $ \\beta $ is, the more noisy the signal. Thats because the lower the beta the less we are taking into account the more stable past values, instead changing the moving avareg alot based on the most recent volatile new piece of data. With higher beta we are weighing the past known and now static values more heavily, thereby inducing a smoother curve.\n",
    "\n",
    "However, and im sorry for doing this, but with respect to GAE, the situation is reversed int both ways, from the example shown in the graph. So why did I show it to you? Well the example is easier to understand and the relationship is similar only reversed, and the relationship is harder to describe. But once you see that relationship, I think its easier to take the inverse of a relatshiption you do understand, than to explain the relationship is a more confusing setting. \n",
    "\n",
    "Whereas in typical times series EWMA, the most immediate data is the most recently data in the past and the other data is the data in the farther past, in GAE the most immediate data is the next reward and the other data is the rewards we are estimating we might get in the future, via the state value function V(s). The $ \\lambda $ is therefore higher when you want to weight these far future estimates higher at the expense of the ones in your immediate future which are more certain. \n",
    "\n",
    "$\\hat{A}_t^{GAE(\\gamma,\\lambda)}$  is the generalized advantage estimator. This [Blog on GAE](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/) explains it well. The higher lambda is the more future steps (k's) you are taking into account\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{GAE(\\gamma,\\lambda)} &= (1-\\lambda)\\Big(\\hat{A}_{t}^{(1)} + \\lambda \\hat{A}_{t}^{(2)} + \\lambda^2 \\hat{A}_{t}^{(3)} + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\Big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V)+ \\cdots \\Big)  \\\\\n",
    "&= (1-\\lambda)\\Big( \\delta_t^V(1+\\lambda+\\lambda^2+\\cdots) + \\gamma\\delta_{t+1}^V(\\lambda+\\lambda^2+\\cdots) + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\left(\\delta_t^V \\frac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V\\frac{\\lambda}{1-\\lambda} + \\cdots\\right) \\\\\n",
    "&= \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^{V}\n",
    "\\end{align}\n",
    "\n",
    "***\n",
    "The tradeoff here is that the estimators $A^{(k)}_{t}$ with small k have low variance but high bias, whereas those with large k have low bias but high variance. Why?\n",
    "\n",
    "I think of it based on the number of terms. With small k, we have fewer terms to sum over (which means low variance). However, the bias is relatively large because it does not make use of extra “exact” information with r_K for K > k\n",
    "\n",
    "Here’s another way to think of it as emphasized in the paper: V(s_t)\n",
    "is constant among the estimator class, so it does not affect the relative bias or variance among the estimators: differences arise entirely due to the k -step returns.\n",
    "***\n",
    "\n",
    "In RL and machine learning, we are calling this noise, the variance, as in the bias variance tradeoff.\n",
    "\n",
    "I like to sum this up as \"the L in lambda for for longtermism\" and depending on the choices we make today, there are many variants of the future we could end up in, so larges L means more longterms and more variance. \n",
    "\n",
    "Basically like many tradeoffs there exists a point of balance for your particular problem. Like in the below example, you get bad learning not only when lambda is too high, but also when it is too low.\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/ca11ba7b2991fe07b7a99b3a3aeba2486ed36261/9-Figure4-1.png\">\n",
    "\n",
    "Im going to rewrite the set of equations above to better mirror the code that we will implement below;\n",
    "\n",
    "first lets add the lambda term\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{A}_t^{(1)} &= \\delta^{V}_{t} = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(2)} &= \\delta^{V}_{t} + (\\gamma \\lambda) \\delta^{V}_{t+1} \\\\\n",
    "\\hat{A}_t^{(k)} &= \\sum_{l=0}^{l=k} (\\gamma \\lambda)^l \\delta_{t+l}^{V}\n",
    "\\end{align}\n",
    "\n",
    "next, rewrite ${A}_t^{(T - t)}$ in terms of t + 1, so that we can calculate the GAE for each\n",
    "step t in the sequence from the last T to the first 0:\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^{V}_{t} &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "\\hat{A}_t^{(T - t)} &= \\delta^{V}_{t} + (\\gamma \\lambda) \\delta^{V}_{t+1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The above two expressions correspond to line 2 and 3 below respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aedd6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_len 9\n",
      "8 0.0 tensor([-1.0153])\n",
      "7 tensor([-0.0697]) tensor([-0.0303])\n",
      "6 tensor([-1.1808]) tensor([-0.5062])\n",
      "5 tensor([-0.7225]) tensor([-0.8080])\n",
      "4 tensor([-0.3859]) tensor([0.1678])\n",
      "3 tensor([-1.6921]) tensor([-1.5018])\n",
      "2 tensor([0.1570]) tensor([-1.0296])\n",
      "1 tensor([-0.2074]) tensor([-0.8508])\n",
      "0 tensor([-0.2203]) tensor([0.0482])\n",
      " \n",
      "[tensor([-1.0153]), tensor([-0.0303]), tensor([-0.5062]), tensor([-0.8080]), tensor([0.1678]), tensor([-1.5018]), tensor([-1.0296]), tensor([-0.8508]), tensor([0.0482])]\n",
      " \n",
      "tensor([[ 0.0482, -0.8508, -1.0296, -1.5018,  0.1678, -0.8080, -0.5062, -0.0303,\n",
      "         -1.0153]])\n"
     ]
    }
   ],
   "source": [
    "gen_len = rewards_.shape[-1]\n",
    "print('gen_len', gen_len)\n",
    "\n",
    "lastgaelam = 0 # lastgaelam takes the role of delta_t+1 in \n",
    "advantages_reversed = []\n",
    "\n",
    "# iterate backwards from last time step of episode, t = T -> 0 \n",
    "for t in reversed(range(gen_len)):\n",
    "    \n",
    "    # 1. V(s_t+1) for all t except t = T\n",
    "    nextvalues = values_[:, t + 1] if t < gen_len - 1 else 0.0  \n",
    "    \n",
    "    # 2. delta_t =  r_t + gamma*V(s_t+1) - V(s_t) \n",
    "    delta = rewards_[:, t] + ppo_trainer.config.gamma * nextvalues - values_[:, t]\n",
    "    \n",
    "    # 3. A_t = delta_t + gamma*lambda*delta_t+1\n",
    "    lastgaelam = delta + ppo_trainer.config.gamma * ppo_trainer.config.lam * lastgaelam\n",
    "    \n",
    "    advantages_reversed.append(lastgaelam)\n",
    "    \n",
    "    print(t, nextvalues, lastgaelam)\n",
    "    \n",
    "print(' ')\n",
    "print(advantages_reversed)\n",
    "print(' ')\n",
    "\n",
    "# reverse advantages_reversed to put it back into forward chronological order then concatenate\n",
    "advantages_ = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
    "\n",
    "print(advantages_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e7f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0171, -0.6342, -0.9339, -1.3950, -0.3255, -0.7251, -0.5340, -0.1604,\n",
      "         -0.7712]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1576, -0.4139, -0.7265, -1.5521,  1.3667, -0.3392,  0.1885,  1.0204,\n",
       "         -0.7015]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since advantage = returns - values_\n",
    "returns_ = advantages_ + values_\n",
    "print(returns_)\n",
    "\n",
    "# whitening simply subtracts the means and divides by the standard deviation to zero mean the data and \n",
    "# impose a std of 1\n",
    "\n",
    "advantages_ = whiten(advantages_)\n",
    "advantages_ = advantages_.detach()\n",
    "\n",
    "advantages_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14aeca",
   "metadata": {},
   "source": [
    "#### Clipping the Value Function (Critic)\n",
    "\n",
    "This implementation detail is not explained in detail in the PPO paper but when you look at the discussion here \n",
    "https://github.com/openai/baselines/issues/91 and the TensorFlow implementation here: https://github.com/openai/baselines/blob/master/baselines/ppo2/model.py\n",
    "\n",
    "The authors add that they\n",
    "\n",
    " *Clip the value function objective to reduce variability during Critic training*\n",
    "\n",
    "here is the actual code from openai baselines\n",
    "\n",
    "```python\n",
    "        # CALCULATE THE LOSS\n",
    "        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n",
    "\n",
    "        # Clip the value to reduce variability during Critic training\n",
    "        # Get the predicted value\n",
    "        vpred = train_model.vf\n",
    "        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)\n",
    "        # Unclipped value\n",
    "        vf_losses1 = tf.square(vpred - R)\n",
    "        # Clipped value\n",
    "        vf_losses2 = tf.square(vpredclipped - R)\n",
    "\n",
    "        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n",
    "```\n",
    " and here are those same operations in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669a58c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "tensor([[-1.1406, -0.2203, -0.2074,  0.1570, -1.6921, -0.3859, -0.7225, -1.1808,\n",
      "         -0.0697]])\n",
      "tensor([[-0.2203, -0.2074,  0.1570, -1.6921, -0.3859, -0.7225, -1.1808, -0.0697,\n",
      "         -3.4668]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.9406, -0.2074, -0.0074, -0.0430, -1.4921, -0.5859, -0.9225, -0.9808,\n",
      "         -0.2697]], grad_fn=<MaximumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(ppo_trainer.config.cliprange_value)\n",
    "print(values_)\n",
    "print(vpred_)\n",
    "\n",
    "vpredclipped_ = \\\n",
    "clip_by_value(vpred_, values_ - ppo_trainer.config.cliprange_value, values_ + ppo_trainer.config.cliprange_value)\n",
    "\n",
    "print(vpredclipped_)\n",
    "\n",
    "# minimize the squared-error loss\n",
    "vf_losses1_ = (vpred_ - returns_) ** 2\n",
    "vf_losses2_ = (vpredclipped_ - returns_) ** 2\n",
    "vf_loss_ = 0.5 * torch.mean(torch.max(vf_losses1_, vf_losses2_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3903ac8",
   "metadata": {},
   "source": [
    "in the TRL version a value called `vf_clipfrac` is calculated to report what proportion of the losses ended up being clipped, so < 0.5 means that most of our our value function losses were within the range.\n",
    "\n",
    "#### Clipped Surrogate Objective\n",
    "\n",
    "The Clipped Surrogate Objective is the main discussion point in the PPO paper\n",
    "\n",
    "here is the main equation from the paper\n",
    "\n",
    "$$ r_t(\\theta) = \\frac{\\pi_\\theta (a_t|s_t)}{\\pi_{\\theta_{old}} (a_t|s_t)} $$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}^{CLIP}(\\theta) =\n",
    " \\mathbb{E}_{a_t, s_t \\sim \\pi_{\\theta{new}}} \\biggl[\n",
    "   min \\Bigl(r_t(\\theta) \\bar{A_t},\n",
    "             clip \\bigl(\n",
    "              r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon\n",
    "             \\bigr) \\bar{A_t}\n",
    "   \\Bigr)\n",
    " \\biggr]\n",
    "\\end{align}\n",
    "\n",
    "In laymens terms, this says:\n",
    "\n",
    "r(theta)_t is the ratio, not reward, between the new policy and old policy. meaning, a ratio of 1 means they are the same with respect to this particular action and state, a ratio < 0.8 or > 1.2 means that the new policy is very different from the old, by a factor of 0.2 in this example.\n",
    "\n",
    "So in order to disincentivize radical noisy unconstructive updates to the new policy (This phrasing is from https://huggingface.co/blog/deep-rl-ppo), we update our policy only if:\n",
    "\n",
    "\n",
    "1. Our ratio is in the range `[1 - epsilon, 1 + epsilon]` \n",
    "\n",
    "OR\n",
    "\n",
    "2. Our ratio is outside the range, but the advantage leads to getting closer to the range. There are 2 cases:\n",
    "    \n",
    "    a. The ratio is below the range but the advantage is > 0\n",
    "    \n",
    "    b. The ratio is above the range but the advantage is < 0.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/blog/assets/93_deep_rl_ppo/recap.jpg\" height=400 width=600>\n",
    "\n",
    "[Table from \"Towards Delivering a Coherent Self-Contained Explanation of Proximal Policy Optimization\" by Daniel Bick](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)\n",
    "\n",
    "here it is in baselines tensorflow\n",
    "\n",
    "```python\n",
    "        # Calculate ratio (pi current policy / pi old policy)\n",
    "        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)\n",
    "\n",
    "        # Defining Loss = - J is equivalent to max J\n",
    "        pg_losses = -ADV * ratio\n",
    "\n",
    "        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)\n",
    "\n",
    "        # Final PG loss\n",
    "        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n",
    "        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))\n",
    "        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))\n",
    "\n",
    "        # Total loss\n",
    "        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n",
    "```\n",
    "\n",
    "and here in pytorch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f259d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_ = torch.exp(new_logprobs_ - old_logprobs_)\n",
    "pg_losses_ = -advantages_ * ratio_\n",
    "pg_losses2_ = \\\n",
    "    -advantages_ * torch.clamp(ratio_, 1.0 - ppo_trainer.config.cliprange, 1.0 + ppo_trainer.config.cliprange)\n",
    "\n",
    "pg_loss_ = torch.mean(torch.max(pg_losses_, pg_losses2_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ffcbe",
   "metadata": {},
   "source": [
    "#### Combined Objective\n",
    "\n",
    "<img src=\"https://huggingface.co/blog/assets/93_deep_rl_ppo/ppo-objective.jpg\" height=400 width=600> \n",
    "\n",
    "The TRL authors leave out the entropy term because there does not seems to be a way to do it such that it gives much benefit at this time https://github.com/lvwerra/trl/issues/131, which is why in our case you see that after we return the policy gradient loss and value function loss from the function above, the final loss we backpropagate on is\n",
    "\n",
    "`loss = pg_loss + self.config.vf_coef * vf_loss`\n",
    "\n",
    "We maximize this objective, this is why there is a negative sign for the squared-error loss because maximizing the negative of this component is the same as minimizing the original component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d2eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
