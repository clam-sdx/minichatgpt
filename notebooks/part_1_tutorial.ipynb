{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3119ced",
   "metadata": {},
   "source": [
    "# Mini-ChatGPT\n",
    "\n",
    "This tutorial assumes the reader has some basic machine learning (ML) and natural language processing (NLP) knowledge, but is new to training large language models, aka foundation models, aka transformer neural networks, using reinforcement learning. If you know what a loss aka cost aka objective function is and how a token aka input_id represents a vector aka embedding, you should be good.\n",
    "\n",
    "If not here is a really great place to start  https://lena-voita.github.io/nlp_course/language_modeling.html \n",
    "\n",
    "Special thanks to Leandro von Werra 's https://github.com/lvwerra/trl repo\n",
    "\n",
    "diagram from https://openai.com/blog/chatgpt/ ChatGPT is an improved InstructGPT \n",
    "\n",
    "<img src='https://cdn.openai.com/chatgpt/draft-20221129c/ChatGPT_Diagram.svg'>\n",
    "\n",
    "#### Figure 1\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Supervised Fine-Tuning (SFT) is the process of taking a model already pre-trained on a very large corpus of data, and re-training it carefully as not to erase the knowledge the model has gained from that pre-training [catastropic forgetting](https://github.com/clam004/intro_continual_learning), but rather slightly nudge the model to perform better in a more specific or narrow domain represented by a much smaller fine tuning dataset.\n",
    "\n",
    "There are already plenty of resources on Supervised Fine-Tuning (SFT) aka fine-tuning already, some with impressively good explanations, mistakenly claiming to help you build a mini-chatgpt. I will not cover SFT, instead I will point you to some of my favorite resources. \n",
    "\n",
    "Here is an explanatory video by Huggingface of the Decoder, or autoregressive, transformers like GPT: https://youtu.be/d_ixlCubqQw\n",
    "\n",
    "Here is a great blog explaining language modeling and the cross entropy loss function https://lena-voita.github.io/nlp_course/language_modeling.html , I would start here if you have done ML but not deep learning for NLP before. \n",
    "\n",
    "Fine-tuning GPT-like models: https://huggingface.co/course/chapter7/6?fw=pt and [Guide to fine-tuning Text Generation models: GPT-2, GPT-Neo and T5](https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e)\n",
    "\n",
    "ChatGPT likely was fine-tuned on a dataset of instructions and examples of following to\n",
    " \n",
    "### Step 2\n",
    "\n",
    "Fine-tuning usually means training for only a small number of epochs with a much smaller learning rate, but also using the cross entropy loss function used to do pre-training, but this way to learning is abit limited, can you tell why? \n",
    "\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/lang_models/neural/one_step_loss_intuition-min.png\">\n",
    "\n",
    "#### Figure 2\n",
    "\n",
    "This example of using cross entropy shows how the training algorithm rewards or penalizes based on how large the logit (probability mass asigned to) for \"cat\" is, just this one way of completing the sequence, but we know that in reality when generating responses to instructions, there isnt one right ways to do it even in the cases when there is one right answer, there are many ways to be good and many ways to be bad. \n",
    "\n",
    "For the sake of keeping this tutorial light weight, fit on commonly available compute and intuitive, we have simplified the overall strategy in 2 major ways:\n",
    "\n",
    "A. ChatGPT and InstructGPT, although they are very specially trained with reinforcement learning (RL), when you generalize, ChatGPT is just taking some prompt, aka input text (the phrase or instructions) and generating an output, aka continuing that text (providing a response or answer). It takes alot more compute memory to represent these long instructions and long answers in the neural network transformers, so instead we simplify the input text to the start of a movie review (first few words or subwords) and simplify the continuing text to the continuation of those first few words.\n",
    "\n",
    "B. The reward modeling has been simplified as compared to InstructGPT https://openai.com/blog/instruction-following/ which is the model that ChatGPT is a scaled up and improved version of. \n",
    "\n",
    "Instead of training a reward model based on human rankings of output text, which was done to make the reward signal more stable or reliable, we use the more direct tactic of using the logits from a classifier (like BERT) as the reward signal, \"positive meanss do more like this, negative means do less like this\", where positive means the review is a referring to movie the reviewer thinks is good (a positive review). \n",
    "\n",
    "### Step 3\n",
    "\n",
    "Given our limited time, I am going to focus on giving you what is hard to find. That is, good explainations of the parts that are usually glossed over yet important, or those parts that are usually explained in a much more jargony, field specific, technical or mathematical manner.\n",
    "\n",
    "That means we will be focusing on step 3 in Figure 1. Namely, how reinforcement learning, something we have seen thus far in the popular media mostly applied to computer games, instead applied to natural language, or the generation of sequences of discrete tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb6d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# this version of the notebook requires weights and biases\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2172f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from minichatgpt.experiments.imdb import imdb_config, imdb_sent_kwargs\n",
    "from minichatgpt.processdata.build_dataset import build_dataset\n",
    "from minichatgpt import PPOTrainer, LengthSampler, Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c31006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2bd6a5d7d39a840d.arrow\n",
      "Loading cached processed dataset at /Users/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2ecf25d24c93f132.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarsonlam004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/carson/projects/minichatgpt/notebooks/wandb/run-20230205_150125-zy9bbfyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/carsonlam004/trl/runs/zy9bbfyh\" target=\"_blank\">bright-springroll-5</a></strong> to <a href=\"https://wandb.ai/carsonlam004/trl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/carsonlam004/trl\" target=\"_blank\">https://wandb.ai/carsonlam004/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/carsonlam004/trl/runs/zy9bbfyh\" target=\"_blank\">https://wandb.ai/carsonlam004/trl/runs/zy9bbfyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lab = Lab(imdb_config)\n",
    "\n",
    "dataset = lab.build_dataset(dataset_name=\"imdb\",input_min_text_length=2,input_max_text_length=8)\n",
    "new_policy, old_policy, tokenizer = lab.init_policies_tokenizer()\n",
    "ppo_trainer = lab.init_ppo_trainer(imdb_config, new_policy, old_policy, tokenizer, dataset)\n",
    "reward_model = lab.init_reward_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b03293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b81498a2e10422f9d04790517e8a3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the part of each batch are:  dict_keys(['label', 'input_ids', 'query'])\n",
      "--------------------------------------------------\n",
      "each batch has  256 samples\n",
      "--------------------------------------------------\n",
      "here are some samples examples  ['The Good Thing about this movie:', 'Please, someone', 'Craz']\n",
      "--------------------------------------------------\n",
      "here are the token ids of those examples  [tensor([  464,  4599, 21561,   546,   428,  3807,    25]), tensor([5492,   11, 2130]), tensor([  34, 3247])]\n"
     ]
    }
   ],
   "source": [
    "for batch_step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    \n",
    "    query_tensors = batch['input_ids']\n",
    "    \n",
    "    break\n",
    "    \n",
    "print('the part of each batch are: ', batch.keys())\n",
    "print('-'*50)\n",
    "print('each batch has ', len(batch['query']), 'samples')\n",
    "print('-'*50)\n",
    "print('here are some samples examples ', batch['query'][:3])\n",
    "print('-'*50)\n",
    "print('here are the token ids of those examples ', batch['input_ids'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58662f8c",
   "metadata": {},
   "source": [
    "As you can see above, the text is our samples is represented by the token IDs in input_ids, and each token is either a word or a subword. Roughly speaking the average token represents a word or subword taht is around 3 characters long, 'can', 'ed', 'con', 'tion' etc. Because of the code `input_min_text_length=2, input_max_text_length=8` above in `dataset = lab.build_dataset()`, you will find a random assortment of sample lengths in each batch, but they will be no less than 2 tokens short and no more than 8 tokens long. \n",
    "\n",
    "In reinforcement learning the agent acts in an environment, like a computer game, and takes actions, like hiting the jump button or moving to the left, which in the future may result in more or less reward, like points in a game. Here the environment is the random assortmen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb90d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
