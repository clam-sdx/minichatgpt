{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52adb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "print ('number of GPUs:', torch.cuda.device_count())\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from minichatgpt.experiments.imdb import config, sent_kwargs\n",
    "from minichatgpt import Lab, PPOConfig\n",
    "from minichatgpt.processdata.collators import imdb_dataloader_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9365b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# For the sake of the speed of this demonstration, the batch_size is temporarily decreased from 256 to 4\n",
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\", # \"distilgpt2\", #\n",
    "    learning_rate= 1.41e-5, # 1e-4,\n",
    ")\n",
    "\n",
    "#batch_size = 256\n",
    "#config.batch_size = batch_size\n",
    "#config.forward_batch_size = batch_size//8\n",
    "\n",
    "print(config.batch_size)\n",
    "print(config.forward_batch_size)\n",
    "\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True, # need to do this for output[1][\"score\"] to be the positive score\n",
    "    #\"top_k\":None, # dont do this or the index of the score will not always be positive second\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": config.forward_batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84bda234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-a680d0720b68191d.arrow\n",
      "Loading cached processed dataset at /home/carson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ec21da66149d9ccd.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124.440577"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab = Lab(config)\n",
    "\n",
    "dataset = lab.build_dataset(dataset_name=\"imdb\",input_min_text_length=2,input_max_text_length=8)\n",
    "\n",
    "new_policy, old_policy, tokenizer = lab.init_policies_tokenizer()\n",
    "\n",
    "lab.set_generation_config(do_sample=True,output_min_length=4,output_max_length=16,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "ppo_trainer = lab.init_ppo_trainer(\n",
    "    config, \n",
    "    new_policy, \n",
    "    old_policy, \n",
    "    tokenizer, \n",
    "    dataset, \n",
    "    dataloader_collator = imdb_dataloader_collator,\n",
    ")\n",
    "\n",
    "reward_model = lab.init_reward_model()\n",
    "\n",
    "lab.num_params_million(new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7dc5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carson/Desktop/Projects/minichatgpt/venv/lib/python3.7/site-packages/transformers/pipelines/text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "device_map = {\n",
    "  0: [ 0, 1, 2, 3, 4, 5],\n",
    "  1: [ 6, 7, 8, 9, 10, 11],\n",
    "}\n",
    "\n",
    "new_policy.pretrained_model.parallelize(device_map)\n",
    "old_policy.pretrained_model.parallelize(device_map)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "returns_list = []\n",
    "rewards_list = []\n",
    "\n",
    "print(new_policy.pretrained_model.device)\n",
    "#print(new_policy.v_head)\n",
    "#new_policy.v_head.summary.to('cuda:1')\n",
    "print(new_policy.v_head.summary.weight.device)\n",
    "#lab.reward_model.model.to('cuda:1')\n",
    "#lab.reward_model.device = torch.device('cuda:1')\n",
    "print(lab.reward_model.model.device)\n",
    "\n",
    "rewards_dict = \\\n",
    "lab.reward_model(['great outstanding an fun movie','boring pretentious script'], **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "023d8d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': -2.4846694469451904},\n",
       "  {'label': 'POSITIVE', 'score': 2.7756235599517822}],\n",
       " [{'label': 'NEGATIVE', 'score': 2.587322235107422},\n",
       "  {'label': 'POSITIVE', 'score': -2.987480401992798}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a3c622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': -2.987480401992798}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_dict[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15104fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward -1.3246885630651377\n",
      "reward -1.2885815703339176\n",
      "reward -1.3285888170066755\n",
      "reward -1.259353297224152\n",
      "reward -1.2646142966113985\n",
      "reward -1.2644752841733862\n",
      "reward -1.1858413858572021\n",
      "reward -1.1770091366197448\n",
      "reward -1.1213959993547178\n",
      "reward -1.072699697659118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carson/Desktop/Projects/minichatgpt/venv/lib/python3.7/site-packages/transformers/pipelines/base.py:1048: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward -1.2004812633967958\n",
      "reward -1.0589635631476995\n",
      "reward -1.0908369235985447\n",
      "reward -1.1937765949114691\n",
      "reward -1.0539054316614056\n",
      "reward -1.1886218634535908\n",
      "reward -1.1640615994983818\n",
      "reward -1.106528560470906\n",
      "reward -1.164456450176658\n",
      "reward -1.0846820635779295\n",
      "reward -1.0708716130175162\n",
      "reward -1.0277549654520044\n",
      "reward -1.1003903613745933\n",
      "reward -1.1289538597266073\n",
      "reward -1.1599947292124853\n",
      "reward -1.1293962329800706\n",
      "reward -1.0684708553999371\n",
      "reward -1.097040847525932\n",
      "reward -1.0858292848570272\n",
      "reward -1.0908920595684322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90771/1320923272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#### Run PPO step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mppo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/minichatgpt/minichatgpt/trainer/ppo_trainer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, queries, responses, scores)\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                 )\n\u001b[1;32m    427\u001b[0m                 \u001b[0mall_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/minichatgpt/minichatgpt/trainer/ppo_trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, old_logprobs, values, rewards, query, response, model_input)\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_p\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/minichatgpt/venv/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/minichatgpt/venv/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/minichatgpt/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config.seed = 0\n",
    "\n",
    "for batch_step, batch in enumerate(ppo_trainer.dataloader):\n",
    "    \n",
    "    queries = batch['input_ids']\n",
    "    \n",
    "    #### Get response from gpt2\n",
    "    responses = []\n",
    "    for query in queries:\n",
    "        gen_len = lab.output_length_sampler()\n",
    "        lab.generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **lab.generation_kwargs)\n",
    "        responses.append(response.squeeze()[-gen_len:])\n",
    "        \n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in responses]\n",
    "    \n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q,r in zip(batch['query'], batch['response'])]\n",
    "    pipe_outputs = lab.reward_model(texts, **sent_kwargs)\n",
    "    \n",
    "    scores = [output[1][\"score\"] for output in pipe_outputs]\n",
    "    rewards_list.append(np.mean(scores))\n",
    "    print('reward', rewards_list[-1])\n",
    "    \n",
    "    rewards = [torch.tensor(score) for score in scores]\n",
    "    #rewards = [r.to(new_policy.pretrained_model.device) for r in rewards]\n",
    "    #rewards = [r.to(new_policy.v_head.summary.weight.device) for r in rewards]\n",
    "    \n",
    "    #### Run PPO step \n",
    "    stats = ppo_trainer.step(queries, responses, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    returns_list.append(stats['ppo/returns/mean'])\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a91806",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Training Iterations (minibatches)', fontsize=12)\n",
    "plt.ylabel('Mean Sum of Rewards (per Sequence)', fontsize=12)\n",
    "#plt.plot(returns_list, label='returns')\n",
    "plt.plot(rewards_list, label='rewards')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3baf15ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This film has gone through another version without a',\n",
       " 'I thought this was logical. Then I',\n",
       " 'On Sunday July 27, 1997, while visiting Dudley, a local TV reporter was interviewing dif']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f942268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 0.4410437047481537},\n",
       "  {'label': 'POSITIVE', 'score': -0.6981233358383179}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.12310316413640976},\n",
       "  {'label': 'POSITIVE', 'score': -0.176642045378685}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.09334967285394669},\n",
       "  {'label': 'POSITIVE', 'score': -0.22255778312683105}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_outputs[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f7ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
